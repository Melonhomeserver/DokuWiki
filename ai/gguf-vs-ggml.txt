====== GGUF vs GGML ======

GGUF and GGML are file formats for quantized models. To interact with these files, you need to use ''llama.cpp''. These formats enable efficient inference from a single file, making the LLM deployment process much more simple and cost-effective.

GGML was an earlier iteration in the evolution of LLM file formats and served as a basis for future improvements. GGMLâ€™s relevance has waned. The primary reasons are its slower processing capabilities and lack of advanced features compared to GGUF. It is important to keep in mind that GGML support is no longer available in llama.cpp. 

GGUF offers numerous quality-of-life improvements, making it unlikely for users to prefer older versions:

    * ğŸ’¾Â **Single-file Deployment:**Â Ensuring ease of distribution and loading without reliance on external files.
    * ğŸ‘ğŸ¼Â **Extensibility:**Â It allows adding new features to GGML-based executors and information to GGUF models, maintaining compatibility with existing models.
    * ğŸ—ºï¸Â **mmap compatibility:**Â Models can be loaded using memory mapping for enhanced speed in loading and saving.
    * ğŸ˜ŠÂ **User-friendly Design:**Â Simplified model loading and saving process, eliminating the need for external libraries.
    * ğŸ’¿Â **Comprehensive Information Storage:**Â GGUF files contain all necessary data to load a model, requiring no additional input from the user.
    * ğŸ“ŠÂ **Quantization Compatibility:**Â GGUF supports quantization. In this process, model weights, typically stored as 16-bit floating point numbers, are scaled down (e.g., to 4-bit integers) to save computational resources without significantly impacting the modelâ€™s power. This is particularly useful for reducing the demands on expensive GPU RAM.

An essential change in GGUF is adopting a key-value structure for hyperparameters, now known as metadata. This shift permits the addition of new metadata without compromising compatibility with existing models. This structure allows for easy integration of new information, preserving compatibility with existing GGUF models.